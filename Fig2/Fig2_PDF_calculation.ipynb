{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is for the calculation of PDFs in Figure 2 of Rodgers et al. 2021 (https://doi.org/10.5194/esd-2021-50). \n",
    "# If you have have any questions, please contact the author of this notebook.\n",
    "# Author: Lei Huang (huanglei[AT]pusan[DOT]ac[DOT]kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-wright",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "billion-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import glob\n",
    "import dask.array as da\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-parts",
   "metadata": {},
   "source": [
    "# seting parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the mpirun in command line:\n",
    "## mpirun --np 6 dask-mpi --scheduler-file scheduler.json --no-nanny --dashboard-address :8785 --memory-limit=60e9\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file = 'the_path_for_your_scheduler_json_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-problem",
   "metadata": {},
   "source": [
    "# functions for reading ensembles in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "loose-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset prior to concatenation\n",
    "variables = []\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t',\n",
    "            'lon', 'lat', 'gw', 'landfrac', 'area', *variables]\n",
    "def def_process_coords(exceptcv = []):\n",
    "    def process_coords(ds, except_coord_vars=exceptcv):\n",
    "        coord_vars = []\n",
    "        for v in np.array(ds.coords):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        for v in np.array(ds.data_vars):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        return ds.drop(coord_vars)\n",
    "    return process_coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "structural-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to read in files for historical simulations\n",
    "def read_in(var, exceptcv, domain='lnd/', freq='day_1/', stream='h6', chunks=dict(time=365), ens_s = 0, ens_e = 100):\n",
    "    ens_dir = \"mother_directory_for_ensemble_files\"\n",
    "    projens_names = [member.split('archive/')[1][:-1] for member in sorted(\n",
    "        glob.glob(ens_dir + \"b.e21.BSSP370*.f09_g17*/\"))][ens_s:ens_e]\n",
    "    proj_ncfiles = []\n",
    "    for i in np.arange(len(projens_names)):\n",
    "        proj_fnames = sorted(glob.glob(\n",
    "            ens_dir + projens_names[i] + \"/\" + domain + \"proc/tseries/\" + freq + \"*\" + stream + var + \"*\"))\n",
    "        proj_ncfiles.append(proj_fnames[-2:])\n",
    "    ens_numbers = [members.split('LE2-')[1]\n",
    "                   for members in projens_names]\n",
    "    proj_ds = xr.open_mfdataset(proj_ncfiles,\n",
    "                                chunks=chunks,\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                                combine='nested',\n",
    "                                concat_dim=[[*ens_numbers], 'time'],\n",
    "                                parallel=True)\n",
    "\n",
    "    ens_ds = proj_ds.rename({'concat_dim': 'ensemble'})\n",
    "    return ens_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-inspector",
   "metadata": {},
   "source": [
    "# PDF for Nino3.4 SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ruled-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in SST for period of 1980-1989\n",
    "variables = ['SST']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.SST.1980*')) \\\n",
    "    + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.SST.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "sst_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "modern-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in SST for period of 2090-2099\n",
    "sst_proj_ds = read_in(var = '.SST.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(time = 365))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "discrete-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the regions for Nino3\n",
    "sst_hist_nino = sst_hist_ds.SST[:,:,...].sel(nlat = slice(168,206), nlon = slice(204,249))\n",
    "sst_proj_nino = sst_proj_ds.SST.sel(nlat = slice(168,206), nlon = slice(204,249), time = slice('2090-01-02','2100-01-01'))\n",
    "# tarea is the cell area on the T-grid of POP2\n",
    "tarea_hist_nino = sst_hist_ds.TAREA.sel(nlat = slice(168,206), nlon = slice(204,249)).broadcast_like(sst_hist_nino).chunk({'time':sst_hist_nino.chunks[1]})\n",
    "tarea_proj_nino = sst_proj_ds.TAREA.sel(nlat = slice(168,206), nlon = slice(204,249),time = slice('2090-01-02','2100-01-01')).broadcast_like(sst_proj_nino).chunk({'time':sst_proj_nino.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bigger-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the PDF for SST in 1980-1989\n",
    "# please refer to the document of dask.array.histogram for more information\n",
    "h_hist_sst_nino_raw, bins_hist_sst_nino_raw = da.histogram(sst_hist_nino,\n",
    "                                                          bins = np.arange(15,40.2,0.2),\n",
    "                                                          weights = tarea_hist_nino,\n",
    "                                                          density = True)\n",
    "h_hist_sst_nino_raw = h_hist_sst_nino_raw.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "french-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the PDF for SST in 2090-2099\n",
    "h_proj_sst_nino_raw, bins_proj_sst_nino_raw = da.histogram(sst_proj_nino,\n",
    "                                                          bins = np.arange(15,40.2,0.2),\n",
    "                                                          weights = tarea_proj_nino,\n",
    "                                                          density = True)\n",
    "h_proj_sst_nino_raw = h_proj_sst_nino_raw.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "guided-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to csv file\n",
    "s1 = np.expand_dims(bins_hist_sst_nino_raw[1:]-0.1, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_sst_nino_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_sst_nino_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "            columns= ['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-forward",
   "metadata": {},
   "source": [
    "# Fire counts in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "novel-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['NFIRE']\n",
    "exceptcv = ['time', 'lat', 'lon', 'landfrac', 'area',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/lnd/proc/tseries/day_1/*.NFIRE.1980*')) \\\n",
    "    + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/lnd/proc/tseries/day_1/*.NFIRE.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "nfire_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                  preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "raised-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfire_proj_ds = read_in(var = '.NFIRE.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'lnd/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h5',\n",
    "                     chunks= dict(time = 365),\n",
    "                     ens_s = 10,\n",
    "                    ens_e = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "subject-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfire_hist_calif = nfire_hist_ds.NFIRE.sel(lat = slice(32,41), lon = slice(235,242))*10000*365*24*3600 # convert the unit to the one shown in Figure 2\n",
    "nfire_proj_calif = nfire_proj_ds.NFIRE.sel(lat = slice(32,41), lon = slice(235,242), time = slice('2090-01-01','2099-12-31'))*10000*365*24*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "alternate-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_hist_calif = nfire_hist_ds.landfrac.sel(lat = slice(32,41), lon = slice(235,242))\n",
    "landfrac_proj_calif = nfire_proj_ds.landfrac.sel(lat = slice(32,41), lon = slice(235,242), time = slice('2090-01-01','2099-12-31'))\n",
    "area_hist_calif = nfire_hist_ds.area.sel(lat = slice(32,41), lon = slice(235,242))\n",
    "area_proj_calif = nfire_proj_ds.area.sel(lat = slice(32,41), lon = slice(235,242), time = slice('2090-01-01','2099-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "crude-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_hist_calif = landfrac_hist_calif.broadcast_like(nfire_hist_calif).chunk({'time':nfire_hist_calif.chunks[1]})\n",
    "landfrac_proj_calif = landfrac_proj_calif.broadcast_like(nfire_proj_calif).chunk({'time':nfire_proj_calif.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "sunset-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_hist_calif = area_hist_calif.broadcast_like(nfire_hist_calif).chunk({'time':nfire_hist_calif.chunks[1]})\n",
    "area_proj_calif = area_proj_calif.broadcast_like(nfire_proj_calif).chunk({'time':nfire_proj_calif.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "derived-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfire_hist_calif = nfire_hist_calif.where(landfrac_hist_calif >= 0.9)\n",
    "nfire_proj_calif = nfire_proj_calif.where(landfrac_proj_calif >= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "green-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_nfire_calif_raw, bins_hist_nfire_calif_raw = np.histogram(nfire_hist_calif,\n",
    "                                                              bins = np.arange(0,2500.1,10),\n",
    "                                                              weights = area_hist_calif,\n",
    "                                                              density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "static-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_nfire_calif_raw, bins_proj_nfire_calif_raw = np.histogram(nfire_proj_calif,\n",
    "                                                              bins = np.arange(0,2500.1,10),\n",
    "                                                              weights = area_proj_calif,\n",
    "                                                              density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "broken-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_nfire_calif_raw[1:] - 5, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_nfire_calif_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_nfire_calif_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-viewer",
   "metadata": {},
   "source": [
    "# PDF for Chlorophyll in NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "headed-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Biogeochemistry module, chlorophyll concentration equals the sum of diatChl_SURF, diazChl_SURF, and spChl_SURF\n",
    "## read in chlorophyll for 1980-1989\n",
    "variables = ['diatChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.diatChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.diatChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "tchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.diazChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.diazChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "variables = ['diazChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "zchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.spChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.spChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "variables = ['spChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "spchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "initial-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in chlorophyll for 2090-2099\n",
    "variables = ['diatChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "tchl_proj_ds = read_in(var = '.diatChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n",
    "variables = ['diazChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "zchl_proj_ds = read_in(var = '.diazChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n",
    "variables = ['spChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "spchl_proj_ds = read_in(var = '.spChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "entitled-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLAT = xr.open_dataset(ncfiles[-1]).TLAT\n",
    "TLONG = xr.open_dataset(ncfiles[-1]).TLONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "duplicate-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_hist = tchl_hist_ds.diatChl_SURF[:,:,...] \\\n",
    "            + zchl_hist_ds.diazChl_SURF[:,:,...] \\\n",
    "                + spchl_hist_ds.spChl_SURF[:,:,...]\n",
    "chl_proj = tchl_proj_ds.diatChl_SURF.sel(time = slice('2090-01-02','2100-01-01')) \\\n",
    "            + zchl_proj_ds.diazChl_SURF.sel(time = slice('2090-01-02','2100-01-01')) \\\n",
    "            + spchl_proj_ds.spChl_SURF.sel(time = slice('2090-01-02','2100-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "together-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea_hist = tchl_hist_ds.TAREA.broadcast_like(chl_hist).chunk({'time':chl_hist.chunks[1]})\n",
    "tarea_proj = tchl_proj_ds.TAREA.sel(time = slice('2090-01-02','2100-01-01')).broadcast_like(chl_proj).chunk({'time':chl_proj.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_hist_NA = chl_hist.where((TLAT>=40) & (TLAT <= 60) & (TLONG >= 300) & (TLONG <= 345), drop = True)\n",
    "chl_proj_NA = chl_proj.where((TLAT>=40) & (TLAT <= 60) & (TLONG >= 300) & (TLONG <= 345), drop = True)\n",
    "tarea_hist_NA = tarea_hist.where((TLAT>=40) & (TLAT <= 60) & (TLONG >= 300) & (TLONG <= 345), drop = True)\n",
    "tarea_proj_NA = tarea_proj.where((TLAT>=40) & (TLAT <= 60) & (TLONG >= 300) & (TLONG <= 345), drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "whole-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_chl_NA_raw, bins_hist_chl_NA_raw = np.histogram(chl_hist_NA,\n",
    "                                                      bins = np.arange(0,20.2,0.2),\n",
    "                                                      weights = tarea_hist_NA,\n",
    "                                                      density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "checked-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_chl_NA_raw, bins_proj_chl_NA_raw = np.histogram(chl_proj_NA,\n",
    "                                                      bins = np.arange(0,20.2,0.2),\n",
    "                                                      weights = tarea_proj_NA,\n",
    "                                                      density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "sharing-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_chl_NA_raw[1:]-0.1, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_chl_NA_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_chl_NA_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1), \n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-spending",
   "metadata": {},
   "source": [
    "# NEP in Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "single-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['NEP']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw', 'landfrac', 'area',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/lnd/proc/tseries/day_1/*.NEP.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/lnd/proc/tseries/day_1/*.NEP.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "nep_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operational-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_proj_ds = read_in(var = '.NEP.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'lnd/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h5',\n",
    "                     chunks= dict(time = 365),\n",
    "                     ens_s = 10,\n",
    "                    ens_e = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "changed-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_hist_amazon = nep_hist_ds.NEP.sel(lat = slice(-10,10), lon = slice(280,310))* 1000000\n",
    "nep_proj_amazon = nep_proj_ds.NEP.sel(lat = slice(-10,10), lon = slice(280,310), time = slice('2090-01-01','2099-12-31'))* 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "varying-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_hist_amazon = nep_hist_ds.landfrac.sel(lat = slice(-10,10), lon = slice(280,310))\n",
    "landfrac_proj_amazon = nep_proj_ds.landfrac.sel(lat = slice(-10,10), lon = slice(280,310), time = slice('2090-01-01','2099-12-31'))\n",
    "area_hist_amazon = nep_hist_ds.area.sel(lat = slice(-10,10), lon = slice(280,310))\n",
    "area_proj_amazon = nep_proj_ds.area.sel(lat = slice(-10,10), lon = slice(280,310), time = slice('2090-01-01','2099-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "guided-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_hist_amazon = area_hist_amazon.broadcast_like(nep_hist_amazon).chunk({'time':nep_hist_amazon.chunks[1]})\n",
    "area_proj_amazon = area_proj_amazon.broadcast_like(nep_proj_amazon).chunk({'time':nep_proj_amazon.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "exact-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_hist_amazon = nep_hist_amazon.where(landfrac_hist_amazon[0,...] >= 0.9)\n",
    "nep_proj_amazon = nep_proj_amazon.where(landfrac_proj_amazon[0,...] >= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "compatible-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_nep_amazon_raw, bins_hist_nep_amazon_raw = np.histogram(nep_hist_amazon,\n",
    "                                                              bins = np.arange(-60,60.06,1),\n",
    "                                                              weights = area_hist_amazon,\n",
    "                                                              density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "appointed-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_nep_amazon_raw, bins_proj_nep_amazon_raw = np.histogram(nep_proj_amazon,\n",
    "                                                              bins = np.arange(-60,60.06,1),\n",
    "                                                              weights = area_proj_amazon,\n",
    "                                                              density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "framed-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_nep_amazon_raw[1:] - 0.5, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_nep_amazon_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_nep_amazon_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-management",
   "metadata": {},
   "source": [
    "# PDF for precipitation in Nino3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "italian-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['PRECT']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/atm/proc/tseries/day_1/*.PRECT.1980*')) \\\n",
    "    + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/atm/proc/tseries/day_1/*.PRECT.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "prect_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                  preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "younger-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "prect_proj_ds = read_in(var = '.PRECT.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'atm/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h1',\n",
    "                     chunks= dict(time = 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "parallel-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "prect_hist_nino = prect_hist_ds.PRECT.sel(lat = slice(-5,5), lon = slice(190,240))* 24* 3600* 1000\n",
    "prect_proj_nino = prect_proj_ds.PRECT.sel(lat = slice(-5,5), lon = slice(190,240), time = slice('2090-01-01','2099-12-31'))* 24* 3600* 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "collected-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_prect_nino_raw, bins_hist_prect_nino_raw = np.histogram(prect_hist_nino,\n",
    "                                                            bins = np.arange(0,1000.01,4),\n",
    "                                                            weights = gw_hist_nino,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "pacific-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_prect_nino_raw, bins_proj_prect_nino_raw = np.histogram(prect_proj_nino,\n",
    "                                                            bins = np.arange(0,1000.01,4),\n",
    "                                                            weights = gw_proj_nino,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "furnished-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_prect_nino_raw[1:] - 2, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_prect_nino_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_prect_nino_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b1752477267ee97aeec3d3273a79c9e2fc00a2b1bc5391b7f8695748b7472b2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "6484171234cdd38788da5cb2d28a85e0dc377a9d1bd3aff49df05eb5ce254897"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
