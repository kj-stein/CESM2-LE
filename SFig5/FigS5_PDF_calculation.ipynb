{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is for the calculation of PDFs in Figure S5 of Rodgers et al. 2021 (https://doi.org/10.5194/esd-2021-50). \n",
    "# If you have have any questions, please contact the author of this notebook.\n",
    "# Author: Lei Huang (huanglei[AT]pusan[DOT]ac[DOT]kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-antibody",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "improving-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import glob\n",
    "import dask.array as da\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-lover",
   "metadata": {},
   "source": [
    "# seting parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the mpirun in command line:\n",
    "## mpirun --np 6 dask-mpi --scheduler-file scheduler.json --no-nanny --dashboard-address :8785 --memory-limit=60e9\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file = 'the_path_for_your_scheduler_json_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-james",
   "metadata": {},
   "source": [
    "# functions for reading ensembles in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "excessive-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset prior to concatenation\n",
    "variables = []\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t',\n",
    "            'lon', 'lat', 'gw', 'landfrac', 'area', *variables]\n",
    "def def_process_coords(exceptcv = []):\n",
    "    def process_coords(ds, except_coord_vars=exceptcv):\n",
    "        coord_vars = []\n",
    "        for v in np.array(ds.coords):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        for v in np.array(ds.data_vars):\n",
    "            if not v in except_coord_vars:\n",
    "                coord_vars += [v]\n",
    "        return ds.drop(coord_vars)\n",
    "    return process_coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "english-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to read in files for historical simulations\n",
    "def read_in(var, exceptcv, domain='lnd/', freq='day_1/', stream='h6', chunks=dict(time=365), ens_s = 0, ens_e = 100):\n",
    "    ens_dir = \"mother_directory_for_ensemble_files\"\n",
    "    projens_names = [member.split('archive/')[1][:-1] for member in sorted(\n",
    "        glob.glob(ens_dir + \"b.e21.BSSP370*.f09_g17*/\"))][ens_s:ens_e]\n",
    "    proj_ncfiles = []\n",
    "    for i in np.arange(len(projens_names)):\n",
    "        proj_fnames = sorted(glob.glob(\n",
    "            ens_dir + projens_names[i] + \"/\" + domain + \"proc/tseries/\" + freq + \"*\" + stream + var + \"*\"))\n",
    "        proj_ncfiles.append(proj_fnames[-2:])\n",
    "    ens_numbers = [members.split('LE2-')[1]\n",
    "                   for members in projens_names]\n",
    "    proj_ds = xr.open_mfdataset(proj_ncfiles,\n",
    "                                chunks=chunks,\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                                combine='nested',\n",
    "                                concat_dim=[[*ens_numbers], 'time'],\n",
    "                                parallel=True)\n",
    "\n",
    "    ens_ds = proj_ds.rename({'concat_dim': 'ensemble'})\n",
    "    return ens_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-literacy",
   "metadata": {},
   "source": [
    "# PDF for precipitation in india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "necessary-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['PRECT']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/atm/proc/tseries/day_1/*.PRECT.1980*')) \\\n",
    "    + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/atm/proc/tseries/day_1/*.PRECT.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "prect_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                  preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contained-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "prect_proj_ds = read_in(var = '.PRECT.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'atm/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h1',\n",
    "                     chunks= dict(time = 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "manufactured-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the purpose to read in NEP here is to get continent mask from CLM5, and clip continent from the precipitation array\n",
    "variables = ['NEP']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw', 'landfrac', 'area',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/lnd/proc/tseries/day_1/*.NEP.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/lnd/proc/tseries/day_1/*.NEP.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "nep_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n",
    "\n",
    "nep_proj_ds = read_in(var = '.NEP.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'lnd/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h5',\n",
    "                     chunks= dict(time = 365),\n",
    "                     ens_s = 10,\n",
    "                    ens_e = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "global-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "prect_hist_india = prect_hist_ds.PRECT.sel(lat = slice(7,30), lon = slice(68,89))* 24* 3600 * 1000\n",
    "prect_proj_india = prect_proj_ds.PRECT.sel(lat = slice(7,30), lon = slice(68,89), time = slice('2090-01-01','2099-12-31'))* 24* 3600 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "heated-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_hist_india = prect_hist_ds.gw.sel(lat = slice(7,30)).broadcast_like(prect_hist_india).chunk({'time':prect_hist_india.chunks[1]})\n",
    "gw_proj_india = prect_proj_ds.gw.sel(lat = slice(7,30), time = slice('2090-01-01','2099-12-31')).broadcast_like(prect_proj_india).chunk({'time':prect_proj_india.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "inclusive-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_hist_india = nep_hist_ds.landfrac.sel(lat = slice(7,30), lon = slice(68,89))\n",
    "landfrac_proj_india = nep_proj_ds.landfrac.sel(lat = slice(7,30), lon = slice(68,89))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "everyday-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "landfrac_hist_india['lat'] = prect_hist_india.lat\n",
    "landfrac_hist_india['lon'] = prect_hist_india.lon\n",
    "landfrac_proj_india['lat'] = prect_proj_india.lat\n",
    "landfrac_proj_india['lon'] = prect_proj_india.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "academic-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip the continent from the precipitation array, we select grids with land fraction over 0.9 as the continent\n",
    "prect_hist_india = prect_hist_india.where(landfrac_hist_india[0,...] >= 0.9)\n",
    "prect_proj_india = prect_proj_india.where(landfrac_proj_india[0,0,...]>= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "congressional-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_prect_india_raw, bins_hist_prect_india_raw = np.histogram(prect_hist_india,\n",
    "                                                            bins = np.arange(0,600.1,4),\n",
    "                                                            weights = gw_hist_india,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "intimate-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_prect_india_raw, bins_proj_prect_india_raw = np.histogram(prect_proj_india,\n",
    "                                                            bins = np.arange(0,600.1,4),\n",
    "                                                            weights = gw_proj_india,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "detected-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_prect_india_raw[1:]-2, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_prect_india_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_prect_india_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-intent",
   "metadata": {},
   "source": [
    "# PDF for Arctic Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "soviet-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['TS']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/atm/proc/tseries/day_1/*.TS.1980*')) \\\n",
    "    + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/atm/proc/tseries/day_1/*.TS.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "ts_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                               preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "funny-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_proj_ds = read_in(var = '.TS.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'atm/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h1',\n",
    "                     chunks= dict(time = 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "illegal-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_hist_Ac = ts_hist_ds.TS.sel(lat = slice(60,90)) - 273.15\n",
    "ts_proj_Ac = ts_proj_ds.TS.sel(lat = slice(60,90), time = slice('2090-01-01','2099-12-31')) - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "muslim-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_hist_Ac = ts_hist_ds.gw.sel(lat = slice(60,90)).broadcast_like(ts_hist_Ac).chunk({'time':ts_hist_Ac.chunks[1]})\n",
    "gw_proj_Ac = ts_proj_ds.gw.sel(lat = slice(60,90), time = slice('2090-01-01','2099-12-31')).broadcast_like(ts_proj_Ac).chunk({'time':ts_hist_Ac.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "german-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_ts_Ac_raw, bins_hist_ts_Ac_raw = np.histogram(ts_hist_Ac,\n",
    "                                                    bins = np.arange(-50,40.5,0.5 ),\n",
    "                                                    weights = gw_hist_Ac,\n",
    "                                                    density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "unlikely-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_ts_Ac_raw, bins_proj_ts_Ac_raw = np.histogram(ts_proj_Ac,\n",
    "                                                    bins = np.arange(-50,40.5,0.5 ),\n",
    "                                                    weights = gw_proj_Ac,\n",
    "                                                    density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "included-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_ts_Ac_raw[1:]-0.25, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_ts_Ac_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_ts_Ac_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-sensitivity",
   "metadata": {},
   "source": [
    "# PDF for AMOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "played-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['MOC']\n",
    "exceptcv = ['time', 'moc_comp', 'transport_reg', 'lat_aux_grid',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/month_1/*.MOC.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/month_1/*.MOC.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "amoc_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          combine='nested',\n",
    "                                 preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fantastic-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "amoc_proj_ds = read_in(var = '.MOC.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'month_1/',\n",
    "                     stream = 'h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "liable-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "amoc_hist = amoc_hist_ds.MOC.isel(moc_comp = 0, transport_reg = 1).sel(lat_aux_grid = [26.5], method = 'nearest').max('moc_z').mean('lat_aux_grid').compute()\n",
    "amoc_proj = amoc_proj_ds.MOC.isel(moc_comp = 0, transport_reg = 1).sel(lat_aux_grid = [26.5], method = 'nearest').sel(time = slice('2090-02-01','2100-01-01')).max('moc_z').mean('lat_aux_grid').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "endangered-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_amoc_raw, bins_hist_amoc_raw = np.histogram(amoc_hist,\n",
    "                                                  bins = np.arange(0,30.5,0.5),\n",
    "                                                  density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dedicated-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_amoc_raw, bins_proj_amoc_raw = np.histogram(amoc_proj,\n",
    "                                                  bins = np.arange(0,30.5,0.5),\n",
    "                                                  density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "celtic-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_amoc_raw[1:] - 0.25, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_amoc_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_amoc_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-blogger",
   "metadata": {},
   "source": [
    "# PDF for Nino3.4 U-850hpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "maritime-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['U850']\n",
    "exceptcv = ['time', 'lat', 'lon', 'gw',  *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/atm/proc/tseries/day_1/*.U850.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/atm/proc/tseries/day_1/*.U850.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "u850_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks={'time':365},\n",
    "                          combine='nested',\n",
    "                                 preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "adaptive-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "u850_proj_ds = read_in(var = '.U850.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'atm/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h1',\n",
    "                     chunks= dict(time = 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "behavioral-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "u850_hist_nino = u850_hist_ds.U850.sel(lat = slice(-5,5), lon = slice(190,240))\n",
    "u850_proj_nino = u850_proj_ds.U850.sel(lat = slice(-5,5), lon = slice(190,240), time = slice('2090-01-01','2099-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "personalized-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_hist_nino = u850_hist_ds.gw.sel(lat = slice(-5,5)).broadcast_like(u850_hist_nino).chunk({'time':u850_hist_nino.chunks[1]})\n",
    "gw_proj_nino = u850_proj_ds.gw.sel(lat = slice(-5,5), time = slice('2090-01-01','2099-12-31')).broadcast_like(u850_proj_nino).chunk({'time':u850_proj_nino.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "executed-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_u850_nino_raw, bins_hist_u850_nino_raw = np.histogram(u850_hist_nino,\n",
    "                                                            bins = np.arange(-30,20.5,0.5),\n",
    "                                                            weights = gw_hist_nino,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "enormous-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_u850_nino_raw, bins_proj_u850_nino_raw = np.histogram(u850_proj_nino,\n",
    "                                                            bins = np.arange(-30,20.5,0.5),\n",
    "                                                            weights = gw_proj_nino,\n",
    "                                                            density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "intellectual-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_u850_nino_raw[1:]-0.25, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_u850_nino_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_u850_nino_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1),\n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-encounter",
   "metadata": {},
   "source": [
    "# PDF for Chlorophyll in Southern Ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "supreme-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Biogeochemistry module, chlorophyll concentration equals the sum of diatChl_SURF, diazChl_SURF, and spChl_SURF\n",
    "## read in chlorophyll for 1980-1989\n",
    "variables = ['diatChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.diatChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.diatChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "tchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.diazChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.diazChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "variables = ['diazChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "zchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n",
    "ncfiles = sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTcmip6*/ocn/proc/tseries/day_1/*.spChl_SURF.1980*')) \\\n",
    "            + sorted(glob.glob('mother_directory_for_ensemble_files/b.e21.BHISTsmbb*/ocn/proc/tseries/day_1/*.spChl_SURF.1980*'))\n",
    "hist_ens_numbers = [member.split('LE2-')[1][:8] for member in ncfiles]\n",
    "variables = ['spChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "spchl_hist_ds = xr.open_mfdataset(ncfiles,\n",
    "                          chunks=dict(nlat = 192, nlon = 160, time = 365),\n",
    "                          combine='nested',\n",
    "                          preprocess=def_process_coords(exceptcv),\n",
    "                          concat_dim =[[*hist_ens_numbers]],\n",
    "                          parallel = True).rename({'concat_dim':'ensemble'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "lyric-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in chlorophyll for 2090-2099\n",
    "variables = ['diatChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "tchl_proj_ds = read_in(var = '.diatChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n",
    "variables = ['diazChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "zchl_proj_ds = read_in(var = '.diazChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n",
    "variables = ['spChl_SURF']\n",
    "exceptcv = ['time', 'nlat', 'nlon', 'z_t', 'TAREA', *variables]\n",
    "spchl_proj_ds = read_in(var = '.spChl_SURF.',\n",
    "                     exceptcv = exceptcv,\n",
    "                     domain = 'ocn/',\n",
    "                     freq = 'day_1/',\n",
    "                     stream = 'h*',\n",
    "                     chunks= dict(nlat = 192, nlon = 160, time = 365),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "pleased-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLAT = xr.open_dataset(ncfiles[-1]).TLAT\n",
    "TLONG = xr.open_dataset(ncfiles[-1]).TLONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "qualified-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_hist = tchl_hist_ds.diatChl_SURF[:,:,...] \\\n",
    "            + zchl_hist_ds.diazChl_SURF[:,:,...] \\\n",
    "                + spchl_hist_ds.spChl_SURF[:,:,...]\n",
    "chl_proj = tchl_proj_ds.diatChl_SURF.sel(time = slice('2090-01-02','2100-01-01')) \\\n",
    "            + zchl_proj_ds.diazChl_SURF.sel(time = slice('2090-01-02','2100-01-01')) \\\n",
    "            + spchl_proj_ds.spChl_SURF.sel(time = slice('2090-01-02','2100-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "moving-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "tarea_hist = tchl_hist_ds.TAREA.broadcast_like(chl_hist).chunk({'time':chl_hist.chunks[1]})\n",
    "tarea_proj = tchl_proj_ds.TAREA.sel(time = slice('2090-01-02','2100-01-01')).broadcast_like(chl_proj).chunk({'time':chl_proj.chunks[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_hist_SO = chl_hist.where((TLAT >= -60) & (TLAT <= -40), drop = True)\n",
    "chl_proj_SO = chl_proj.where((TLAT >= -60) & (TLAT <= -40), drop = True)\n",
    "tarea_hist_SO = tarea_hist.where((TLAT >= -60) & (TLAT <= -40), drop = True)\n",
    "tarea_proj_SO = tarea_proj.where((TLAT >= -60) & (TLAT <= -40), drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dominican-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_hist_chl_SO_raw, bins_hist_chl_SO_raw = np.histogram(chl_hist_SO,\n",
    "                                                      bins = np.arange(0,20.2,0.2),\n",
    "                                                      weights = tarea_hist_SO,\n",
    "                                                      density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "greatest-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_proj_chl_SO_raw, bins_proj_chl_SO_raw = np.histogram(chl_proj_SO,\n",
    "                                                      bins = np.arange(0,20.2,0.2),\n",
    "                                                      weights = tarea_proj_SO,\n",
    "                                                      density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "commercial-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.expand_dims(bins_hist_chl_SO_raw[1:]-0.1, axis = 1)\n",
    "s2 = np.expand_dims(h_hist_chl_SO_raw, axis = 1)\n",
    "s3 = np.expand_dims(h_proj_chl_SO_raw, axis = 1)\n",
    "pd.DataFrame(data = np.concatenate((s1,s2,s3), axis = 1), \n",
    "             columns=['bins', 'h_hist', 'h_proj']).to_csv('path_csv_file', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b1752477267ee97aeec3d3273a79c9e2fc00a2b1bc5391b7f8695748b7472b2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "6484171234cdd38788da5cb2d28a85e0dc377a9d1bd3aff49df05eb5ce254897"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
